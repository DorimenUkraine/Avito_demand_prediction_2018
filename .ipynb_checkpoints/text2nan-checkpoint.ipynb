{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling nan values with NN\n",
    "\n",
    "In this competition several important features had a lot of missing values. There are numerous ways to deal with missing values and I learned one more. In this [kernel](https://www.kaggle.com/christofhenkel/text2image-top-1) author offers to train neural nets to predict the missing values.\n",
    "\n",
    "This notebook does the same for several features.\n",
    "\n",
    "Notebook is a little mess, as sometimes I changes old cells when predicting other features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing import text, sequence\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n",
    "import keras.backend as K\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "import time \n",
    "import gc \n",
    "\n",
    "np.random.seed(42)\n",
    "import pickle\n",
    "from keras.layers import Input, Dropout, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, CuDNNGRU\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '6'\n",
    "\n",
    "import threading\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from contextlib import closing\n",
    "cores = 6\n",
    "\n",
    "#import nn_functions\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=0))\n",
    "\n",
    "# Check that GPU is ok.\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train...\n",
      "loading test\n",
      "concat dfs\n"
     ]
    }
   ],
   "source": [
    "text_cols = ['param_1','param_2','param_3','title','description']\n",
    "print('loading train...')\n",
    "train = pd.read_csv('f:/Avito/train.csv', index_col = 'item_id', usecols = text_cols + ['item_id','image_top_1', 'price'])\n",
    "train_indices = train.index\n",
    "print('loading test')\n",
    "test = pd.read_csv('f:/Avito/test.csv', index_col = 'item_id', usecols = text_cols + ['item_id','image_top_1', 'price'])\n",
    "test_indices = test.index\n",
    "print('concat dfs')\n",
    "df = pd.concat([train,test])\n",
    "nan_indices = df[pd.isnull(df['image_top_1'])].index\n",
    "not_nan_indices = df[pd.notnull(df['image_top_1'])].index\n",
    "\n",
    "df = df[pd.notnull(df['image_top_1'])]\n",
    "\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning text\n",
      "concat text\n"
     ]
    }
   ],
   "source": [
    "print('cleaning text')\n",
    "\n",
    "for col in text_cols:\n",
    "    df[col] = df[col].fillna('nan').astype(str).replace('/\\n', ' ').replace('\\xa0', ' ').replace('.', '. ').replace(',', ', ')\n",
    "print('concat text')\n",
    "df['text'] = df[text_cols].apply(lambda x: ' '.join(x), axis=1)\n",
    "df.drop(text_cols,axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing...done. 96.29589247703552\n",
      "   Transforming text to seq...\n",
      "done. 75.03474712371826\n",
      "padding X_train\n",
      "done. 11.59083366394043\n",
      "padding X_nan\n",
      "done. 0.9165129661560059\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_features = 100000 # max amount of words considered\n",
    "max_len = 100 #maximum length of text\n",
    "dim = 100 #dimension of embedding\n",
    "\n",
    "\n",
    "print('tokenizing...', end='')\n",
    "tic = time.time()\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(df['text'].values))\n",
    "toc = time.time()\n",
    "print('done. {}'.format(toc-tic))\n",
    "\n",
    "col = 'text'\n",
    "print(\"   Transforming {} to seq...\".format(col))\n",
    "tic = time.time()\n",
    "df[col] = tokenizer.texts_to_sequences(df[col])\n",
    "toc = time.time()\n",
    "print('done. {}'.format(toc-tic))\n",
    "\n",
    "print('padding X_train')\n",
    "tic = time.time()\n",
    "X_train = pad_sequences(df.loc[not_nan_indices,col], maxlen=max_len)\n",
    "toc = time.time()\n",
    "print('done. {}'.format(toc-tic))\n",
    "\n",
    "print('padding X_nan')\n",
    "tic = time.time()\n",
    "X_nan = pad_sequences(df.loc[nan_indices,col], maxlen=max_len)\n",
    "toc = time.time()\n",
    "print('done. {}'.format(toc-tic))\n",
    "\n",
    "df.drop(['text'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.loc[not_nan_indices,'image_top_1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 100, 100)     10000100    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 100, 128)     63744       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 128)          0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256)          0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 256)          1024        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          65792       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          32896       dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 3067)         395643      dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 10,559,199\n",
      "Trainable params: 10,558,687\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input,PReLU,BatchNormalization, GlobalMaxPooling1D, GlobalAveragePooling1D, CuDNNGRU, Bidirectional, Dense, Embedding\n",
    "from keras.layers import Concatenate, Flatten, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import he_uniform\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "from keras.losses import categorical_crossentropy, sparse_categorical_crossentropy\n",
    "\n",
    "def all_pool(tensor):\n",
    "    avg_tensor = GlobalAveragePooling1D()(tensor)\n",
    "    max_tensor = GlobalMaxPooling1D()(tensor)\n",
    "    res_tensor = Concatenate()([avg_tensor, max_tensor])\n",
    "    return res_tensor\n",
    "\n",
    "def build_model():\n",
    "    inp = Input(shape=(max_len,))\n",
    "\n",
    "    embedding = Embedding(max_features + 1, dim)(inp)\n",
    "    x = Bidirectional(CuDNNGRU(64,return_sequences=True))(embedding)\n",
    "    x = all_pool(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256, activation = 'relu')(x)\n",
    "    x = Dense(128, activation = 'relu')(x)\n",
    "    out = Dense(3067, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=0.0005), loss=sparse_categorical_crossentropy)\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1670998 samples, validate on 185667 samples\n",
      "Epoch 1/10\n",
      "1670998/1670998 [==============================] - 232s 139us/step - loss: 3.2424 - val_loss: 2.7964\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.79640, saving model to model.hdf5\n",
      "Epoch 2/10\n",
      "1670998/1670998 [==============================] - 230s 138us/step - loss: 2.5920 - val_loss: 2.7157\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.79640 to 2.71570, saving model to model.hdf5\n",
      "Epoch 3/10\n",
      "1670998/1670998 [==============================] - 231s 138us/step - loss: 2.4216 - val_loss: 2.68350s - l\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.71570 to 2.68350, saving model to model.hdf5\n",
      "Epoch 4/10\n",
      "1670998/1670998 [==============================] - 230s 138us/step - loss: 2.2850 - val_loss: 2.7528\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/10\n",
      "1670998/1670998 [==============================] - 222s 133us/step - loss: 2.1600 - val_loss: 2.8271\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(patience=2)\n",
    "check_point = ModelCheckpoint('model.hdf5', monitor = \"val_loss\", mode = \"min\", save_best_only = True, verbose = 1)\n",
    "\n",
    "history = model.fit(X_train, y, batch_size = 512, epochs = 10,\n",
    "                verbose = 1, validation_split=0.1,callbacks=[early_stop,check_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {tokenizer.word_index[word]:word for word in tokenizer.word_index}\n",
    "weights = model.layers[1].get_weights()[0]\n",
    "embedding_dict = {}\n",
    "for id in id2word:\n",
    "    if id <= weights.shape[0]-1:\n",
    "        embedding_dict[id2word[id]] = weights[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('embedding_dict.p','wb') as f:\n",
    "    pickle.dump(embedding_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115947/115947 [==============================] - 27s 231us/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_nan,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 0\n",
    "classes = np.zeros(shape=np.argmax(preds,axis = 1).shape)\n",
    "for i in range(preds.shape[0]):\n",
    "    if np.max(preds[i]) > 0.1:\n",
    "        k+=1\n",
    "        classes[i] = np.argmax(preds[i])\n",
    "    else:\n",
    "        classes[i] = np.nan\n",
    "df.loc[nan_indices,'image_top_1'] = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115947, 1)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[train_indices].to_csv('train_image_top_1_features.csv')\n",
    "df.loc[test_indices].to_csv('test_image_top_1_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_indices = df[pd.isnull(df['price'])].index\n",
    "not_nan_indices = df[pd.notnull(df['price'])].index\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning text\n",
      "concat text\n",
      "tokenizing...done. 99.14523243904114\n",
      "   Transforming text to seq...\n",
      "done. 74.05337285995483\n",
      "padding X_train\n",
      "done. 12.088027954101562\n",
      "padding X_nan\n",
      "done. 0.7968735694885254\n"
     ]
    }
   ],
   "source": [
    "print('cleaning text')\n",
    "le = LabelEncoder()\n",
    "for col in text_cols:\n",
    "    df[col] = df[col].fillna('nan').astype(str).replace('/\\n', ' ').replace('\\xa0', ' ').replace('.', '. ').replace(',', ', ')\n",
    "\n",
    "df['text'] = df[text_cols].apply(lambda x: ' '.join(x), axis=1)\n",
    "#df['param_2'] = le.fit_transform(df['param_2'])\n",
    "print('concat text')\n",
    "#text_cols.remove('param_2')\n",
    "df.drop(text_cols,axis = 1, inplace = True)\n",
    "#print(df.head())\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_features = 100000 # max amount of words considered\n",
    "max_len = 100 #maximum length of text\n",
    "dim = 100 #dimension of embedding\n",
    "\n",
    "\n",
    "print('tokenizing...',end='')\n",
    "tic = time.time()\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(df['text'].values))\n",
    "toc = time.time()\n",
    "print('done. {}'.format(toc-tic))\n",
    "#print(df.head())\n",
    "col = 'text'\n",
    "print(\"   Transforming {} to seq...\".format(col))\n",
    "tic = time.time()\n",
    "df[col] = tokenizer.texts_to_sequences(df[col])\n",
    "toc = time.time()\n",
    "print('done. {}'.format(toc-tic))\n",
    "#print(df.head())\n",
    "print('padding X_train')\n",
    "tic = time.time()\n",
    "X_train = pad_sequences(df.loc[not_nan_indices,col], maxlen=max_len)\n",
    "toc = time.time()\n",
    "print('done. {}'.format(toc-tic))\n",
    "#print(df.head())\n",
    "print('padding X_nan')\n",
    "tic = time.time()\n",
    "X_nan = pad_sequences(df.loc[nan_indices,col], maxlen=max_len)\n",
    "toc = time.time()\n",
    "print('done. {}'.format(toc-tic))\n",
    "#print(df.head())\n",
    "df.drop(['text'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop(['image_top_1', 'price'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = df.loc[not_nan_indices,'price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['param_2'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 100, 100)     10000100    input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 100, 128)     63744       embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 128)          0           bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 128)          0           bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256)          0           global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 256)          1024        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 256)          65792       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 128)          32896       dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 1)            129         dense_19[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 10,163,685\n",
      "Trainable params: 10,163,173\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    inp = Input(shape=(max_len,))\n",
    "\n",
    "    embedding = Embedding(max_features + 1, dim)(inp)\n",
    "    x = Bidirectional(CuDNNGRU(64,return_sequences=True))(embedding)\n",
    "    x = all_pool(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256, activation = 'relu')(x)\n",
    "    x = Dense(128, activation = 'relu')(x)\n",
    "    out = Dense(1, activation='relu')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=0.0005), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1706323 samples, validate on 189592 samples\n",
      "Epoch 1/10\n",
      "1706323/1706323 [==============================] - 119s 70us/step - loss: 3718952866894470.5000 - val_loss: 60021826043985.2422\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 60021826043985.23438, saving model to model.hdf5\n",
      "Epoch 2/10\n",
      "1706323/1706323 [==============================] - 118s 69us/step - loss: 3717521127945026.5000 - val_loss: 59634368793052.7656\n",
      "\n",
      "Epoch 00002: val_loss improved from 60021826043985.23438 to 59634368793052.76562, saving model to model.hdf5\n",
      "Epoch 3/10\n",
      "1706323/1706323 [==============================] - 118s 69us/step - loss: 3715581617557711.5000 - val_loss: 59889360126915.0312\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/10\n",
      "1706323/1706323 [==============================] - 118s 69us/step - loss: 3713952755738651.5000 - val_loss: 59899465066413.8203\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(patience=2)\n",
    "check_point = ModelCheckpoint('model.hdf5', monitor = \"val_loss\", mode = \"min\", save_best_only = True, verbose = 1)\n",
    "\n",
    "history = model.fit(X_train, y, batch_size = 512, epochs = 10,\n",
    "                verbose = 1, validation_split=0.1,callbacks=[early_stop,check_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "classes = preds\n",
    "for i in range(preds.shape[0]):\n",
    "    if preds[i] > 0.1:\n",
    "        k+=1\n",
    "        classes[i] = preds[i]\n",
    "    else:\n",
    "        classes[i] = np.nan\n",
    "df.loc[nan_indices,'price'] = classes.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[train_indices].to_csv('train_price_features.csv')\n",
    "df.loc[test_indices].to_csv('test_price_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
